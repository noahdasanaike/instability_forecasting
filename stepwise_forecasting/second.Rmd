---
title: "Neural"
output: html_notebook
---

```{r one, warning=FALSE}
library(tidyverse)
library(WDI)
library(janitor)
library(countrycode)
library(readxl)
library(MASS)
library(dataPreparation)
library(glmnet)
library(rworldmap)

demean.mat <- function(xmat) {
  apply(xmat, 2, function(z) z - mean(z))
}
```

```{r conflict}
conflict_data <- read_csv("conflict_data.csv")

full_conflict <- tibble(country_code = unique(conflict_data$country_code)) %>% 
  slice(rep(1:n(), each = 31)) %>%
  group_by(country_code) %>%
  mutate(year = row_number() + 1988) %>%
  merge(conflict_data, by = c("country_code", "year"), all.x = TRUE) %>%
  rowwise() %>%
  mutate(conflict = mean(c(acled_total, ucdp_total, gdelt_total), na.rm = TRUE)) %>%
  pivot_wider(names_from = event_type, values_from = conflict) %>%
  dplyr::select(-c(acled_total, ucdp_total, gdelt_total, `NA`)) %>%
  drop_na(country_code) %>%
  rename(protest = `14`, assault = `18`, fight = `19`) %>% 
  group_by(country_code, year) %>% 
  summarise_all(funs(first(na.omit(.))))

full_conflict[is.na(full_conflict)] <- 0
```

```{r merge}
pca_data <- full_conflict[3:5]

pcr_matrix <- as.matrix(pca_data)

x <- NULL

for (i in 2:15){
  k <- kmeans(pca_data, i, nstart = 25, iter.max = 30)["tot.withinss"][[1]]
  x <- rbind(x, c(i, k))
}

x <- x %>%
  as.tibble()

ggplot(data = x, aes(x = V1, y = V2)) +
  geom_point()

cluster <- kmeans(pca_data, 5, nstart = 25)

pca <- prcomp(pca_data)

test <- tibble(country_name = full_conflict$country_code,
               year = full_conflict$year,
               pca1 =  pca$x[,1],
               pca2 = pca$x[,2],
               cluster = cluster$cluster)

test %>%
  filter(year == 2016) %>%
  mutate(cluster = as.factor(cluster)) %>%
  ggplot(aes(x = pca1, y = pca2, group = cluster, color = cluster)) +
  geom_text(aes(label = country_name))
```

```{r indicators, warning = FALSE, message = FALSE}
peace <- read_xls("peace_months.xls") %>%
  dplyr::select(year, country, peaceyears) %>%
  mutate(iso3 = countrycode(country, origin = "country.name", destination = "iso3c")) %>%
  drop_na(iso3) %>%
  mutate(year_ahead = ifelse(lead(peaceyears) > 0, 0, 1)) %>%
  mutate(year_ahead = ifelse(is.na(lead(peaceyears)), 1, year_ahead)) %>%
  mutate(current_year = ifelse(peaceyears > 0, 0, 1)) %>%
  mutate(current_year = ifelse(is.na(peaceyears), 1, current_year)) %>%
  dplyr::select(year, year_ahead, iso3, current_year) 

indicators <- WDIsearch()

# new_cache = WDIcache()
# dat = WDI(cache = new_cache, 
#           indicator = c(unlist(indicators[, 1])),
#           country = "all", start = 1989, end = 2019)

test_for_merge <- test %>%
  rename(iso3 = country_name) %>%
  dplyr::select(iso3, cluster, year)

dat <- readRDS("wdi_data.RDS")

dat <- dat %>%
  clean_names() %>% 
  mutate(iso3 = countrycode(iso2c, origin = "iso2c", destination = "iso3c")) %>%
  dplyr::select(-c(country, iso2c))

merged <- merge(dat, peace, by = c("iso3", "year")) %>%
  drop_na(iso3) %>%
  merge(test_for_merge, by = c("iso3", "year"))

nas <- merged %>% keep(~all(is.na(.x))) %>% names

merged <- merged[ , -which(names(merged) %in% c(unlist(nas)))]

merged <- merged %>% 
  drop_na(year_ahead) %>%
  group_by(year, cluster) %>% 
  mutate_each(funs(replace(., which(is.na(.)),
                           mean(., na.rm=TRUE)))) %>%
  group_by(iso3) %>%
  mutate_each(funs(replace(., which(is.nan(.)),
                           mean(., na.rm=TRUE))))

merge_filter <- merged 

no_na <- merge_filter[,3:154]
no_na <- no_na[-c(which_are_bijection(no_na))]

no_na <- no_na %>%
  mutate(year = as.numeric(year)) %>%
  arrange(year)

#################
```

just 2016 vs 2014 to 2016:

2016:

```{r model_one}
train_data <- 1:4816

# testing data is set from 1989 to 2015
test_data <- 4817:5008

x <- no_na

x <- x %>%
  dplyr::select(-c(year_ahead, cluster))

y <- no_na$year_ahead

# alpha_i <- 0
# best_i <- 0
# best_lambda <- 100
# 
# foldid <- sample(1:10, size = length( y[train_data]), replace = TRUE)
# 
#  for (p in 1:10){
#    mod <- cv.glmnet(as.matrix(x[train_data, ]), y[train_data], alpha = alpha_i,
#                     family = "binomial")
#    lambda.min <- mod$lambda.min
# 
#    yhat <- predict(mod, newx = as.matrix(x[test_data, ]),
#  											  s = lambda.min,
#  											  type = "response")
# 
#    pred <- yhat %>% tibble() %>% rename(pred = ".")
# 
#    index = min(pred)
#    best_cutoff <- 0
#    best_accuracy <- 0
# 
#    while (index < max(pred))
#    {
#      cutoff <- pred %>%
#        rowwise() %>%
#        mutate(pred = ifelse(pred > index, 1, 0))
# 
#      accuracy <- mean(cutoff$pred == y[test_data])
# 
#      if (accuracy > best_accuracy){
#        best_cutoff <- index
#        best_accuracy <- accuracy
#      }
# 
#      index = index + 0.01
#    }
# 
#    comparison <- pred %>%
#        rowwise() %>%
#        mutate(pred = ifelse(pred > best_cutoff, 1, 0))
# 
#    comparison$actual <- y[test_data]
# 
#    if (lambda.min < best_lambda){
#        best_i <- alpha_i
#        best_lambda <- lambda.min
#      }
# 
#    alpha_i <- alpha_i + 0.1
#  }

# alpha value determined from cross-validation is 0.9

mod <- cv.glmnet(as.matrix(x[train_data, ]), y[train_data], alpha = 0.9)
lambda.min <- mod$lambda.min

yhat <- predict(mod, newx = as.matrix(x[test_data, ]),
                      s = lambda.min,
                      type = "response")

pred <- yhat %>% tibble() %>% rename(pred = ".")

index = min(pred)

best_cutoff <- 0
best_accuracy <- 0

while (index < 2)
{
  cutoff <- pred %>%
  rowwise() %>%
  mutate(pred = ifelse(pred > index, 1, 0))

  accuracy <- mean(cutoff$pred == y[test_data])

  if (accuracy > best_accuracy){
    best_cutoff <- index
    best_accuracy <- accuracy
  }

  index = index + 0.01
}

# best_cutoff is like 0.4

comparison <- pred %>%
  rowwise() %>%
  mutate(pred = ifelse(pred > 0.78, 1, 0))

comparison$actual <- y[test_data]

mean(comparison$actual == comparison$pred)
```

```{r model}
# training data is set from 1989 to 2013
train_data <- 1:4432

# testing data is set from 2014 to 2016
test_data <- 4433:5008

x <- no_na[, 2:146]

x <- x %>%
  dplyr::select(-c(year_ahead))

y <- merge_filter$year_ahead

# alpha_i <- 0
# best_i <- 0
# best_lambda <- 100
# 
# foldid <- sample(1:10, size = length( y[train_data]), replace = TRUE)
# 
#  for (p in 1:10){
#    mod <- cv.glmnet(as.matrix(x[train_data, ]), y[train_data], alpha = alpha_i,
#                     family = "binomial")
#    lambda.min <- mod$lambda.min
# 
#    yhat <- predict(mod, newx = as.matrix(x[test_data, ]),
#  											  s = lambda.min,
#  											  type = "response")
# 
#    pred <- yhat %>% tibble() %>% rename(pred = ".")
# 
#    index = min(pred)
#    best_cutoff <- 0
#    best_accuracy <- 0
# 
#    while (index < max(pred))
#    {
#      cutoff <- pred %>%
#        rowwise() %>%
#        mutate(pred = ifelse(pred > index, 1, 0))
# 
#      accuracy <- mean(cutoff$pred == y[test_data])
# 
#      if (accuracy > best_accuracy){
#        best_cutoff <- index
#        best_accuracy <- accuracy
#      }
# 
#      index = index + 0.01
#    }
# 
#    comparison <- pred %>%
#        rowwise() %>%
#        mutate(pred = ifelse(pred > best_cutoff, 1, 0))
# 
#    comparison$actual <- y[test_data]
# 
#    if (lambda.min < best_lambda){
#        best_i <- alpha_i
#        best_lambda <- lambda.min
#      }
# 
#    alpha_i <- alpha_i + 0.1
#  }

# alpha value determined from cross-validation is 0.9

mod <- cv.glmnet(as.matrix(x[train_data, ]), y[train_data], alpha = 0.9)
lambda.min <- mod$lambda.min

yhat <- predict(mod, newx = as.matrix(x[test_data, ]),
                      s = lambda.min,
                      type = "response")

pred <- yhat %>% tibble() %>% rename(pred = ".")

index = min(pred)
best_cutoff <- 0
best_accuracy <- 0

comparison <- pred %>%
  rowwise() %>%
  mutate(pred = ifelse(pred > 0.24, 1, 0))

comparison$actual <- y[test_data]

comparison$year <- no_na[4433:5008,]$year

mean(subset(comparison, year == 2014)$actual == subset(comparison, year == 2014)$pred)
mean(subset(comparison, year == 2015)$actual == subset(comparison, year == 2015)$pred)
mean(subset(comparison, year == 2016)$actual == subset(comparison, year == 2016)$pred)
```


```{r heatmaps}

f1score <- function(predicted, y_true) {
  true_p <- sum(predicted==y_true & predicted==1)
  true_n <- sum(predicted==y_true & predicted==0)
  false_p <- sum(predicted==1 & y_true==0)
  false_n <- sum(predicted==0 & y_true==1)
  accuracy <- mean(predicted==y_true)
  precision <- true_p/(true_p+false_p)
  recall <- true_p/(true_p+false_n)
  f1 <- 2/(1/precision + 1/recall)
  return(list(Accuracy=accuracy, Precision=precision, Recall=recall, F1=f1))
}

renderHeatmap <- function(data, year) {
    mapped_data <- data[data$year==year,]
    mapped_data <- joinCountryData2Map(mapped_data, joinCode="ISO3", nameJoinColumn="iso3")
    par(mai=c(1,0,0.5,0),xaxs="i",yaxs="i")
    mapCountryData(mapped_data, nameColumnToPlot = "prediction", catMethod="categorical",
                   mapTitle=paste("Historical Conflict Intensity by Country in ", year, sep=""),
                   missingCountryCol="white")
}


country_preds <- comparison
names(country_preds)[1] = "prediction"
country_preds["iso3"] = arrange(merge_filter, year)$iso3[test_data]

renderHeatmap(country_preds, 2014)

year_filter <- country_preds[country_preds$year==2016,]
f1score(year_filter$prediction, year_filter$actual)
```